{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from tensorflow.io import gfile\n",
    "\n",
    "import tempfile\n",
    "import os\n",
    "from six.moves import urllib\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立語意分割模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DeepLabModel(object):\n",
    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "\n",
    "    def __init__(self, tarball_path):\n",
    "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "        graph_def = None\n",
    "\n",
    "        # Extract frozen graph from tar archive.\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "        tar_file.close()\n",
    "\n",
    "        if graph_def is None:\n",
    "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "\n",
    "    def run(self, image, INPUT_TENSOR_NAME = 'ImageTensor:0', OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'):\n",
    "        \"\"\"Runs inference on a single image.\n",
    "\n",
    "        Args:\n",
    "            image: A PIL.Image object, raw input image.\n",
    "            INPUT_TENSOR_NAME: The name of input tensor, default to ImageTensor.\n",
    "            OUTPUT_TENSOR_NAME: The name of output tensor, default to SemanticPredictions.\n",
    "\n",
    "        Returns:\n",
    "            resized_image: RGB image resized from original input image.\n",
    "            seg_map: Segmentation map of `resized_image`.\n",
    "        \"\"\"\n",
    "        width, height = image.size\n",
    "        target_size = (2049,1025)  # size of Cityscapes images\n",
    "        resized_image = image.convert('RGB').resize(target_size, Image.Resampling.LANCZOS)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "        seg_map = batch_seg_map[0]  # expected batch size = 1\n",
    "        if len(seg_map.shape) == 2:\n",
    "            seg_map = np.expand_dims(seg_map,-1)  # need an extra dimension for cv.resize\n",
    "        seg_map = cv.resize(seg_map, (width,height), interpolation=cv.INTER_NEAREST)\n",
    "        return seg_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'mobilenetv2_coco_cityscapes_trainfine'\n",
    "#MODEL_NAME = 'xception65_cityscapes_trainfine'\n",
    "\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "_MODEL_URLS = {\n",
    "    'mobilenetv2_coco_cityscapes_trainfine':\n",
    "        'deeplabv3_mnv2_cityscapes_train_2018_02_05.tar.gz',\n",
    "    'xception65_cityscapes_trainfine':\n",
    "        'deeplabv3_cityscapes_train_2018_02_06.tar.gz',\n",
    "}\n",
    "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
    "print('downloading model, this might take a while...')\n",
    "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME], download_path)\n",
    "print('download completed! loading DeepLab model...')\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### compute_M: \n",
    "### 拿語意分割模型結果與兩張照片去計算不同label的轉換矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_M(seg_map, source_img_path, target_img_path, sift, bf):\n",
    "    '''\n",
    "    seg_map: segmentation map of the source image, a numpy array of shape (height, width)\n",
    "    source_img_path: path to the source image\n",
    "    target_img_path: path to the target image\n",
    "    sift: cv2.SIFT object\n",
    "    bf: cv2.BFMatcher object\n",
    "    '''\n",
    "\n",
    "    unique_labels = np.unique(seg_map)\n",
    "\n",
    "    source_img = cv.imread(source_img_path)\n",
    "    target_img = cv.imread(target_img_path)\n",
    "\n",
    "    gray_source = cv.cvtColor(source_img, cv.COLOR_BGR2GRAY)\n",
    "    gray_target = cv.cvtColor(target_img, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Function to extract and match features for each label\n",
    "    def compute_transform(label, seg_map, gray_frame_0, gray_frame_16):\n",
    "        mask = seg_map == label\n",
    "        keypoints_0, descriptors_0 = sift.detectAndCompute(gray_frame_0, mask.astype(np.uint8))\n",
    "        keypoints_16, descriptors_16 = sift.detectAndCompute(gray_frame_16, mask.astype(np.uint8))\n",
    "\n",
    "        matches = bf.match(descriptors_0, descriptors_16)\n",
    "        matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "        # only use the top 10% of matches\n",
    "        top_matches = matches[:int(len(matches) * 0.1)]\n",
    "\n",
    "        if len(top_matches) >= 4:  # Minimum number of matches to estimate a transform\n",
    "            pts_0 = np.float32([keypoints_0[m.queryIdx].pt for m in top_matches]).reshape(-1, 1, 2)\n",
    "            pts_16 = np.float32([keypoints_16[m.trainIdx].pt for m in top_matches]).reshape(-1, 1, 2)\n",
    "            # estimate projective transform\n",
    "            M, mask = cv2.estimateAffinePartial2D(pts_0, pts_16)\n",
    "        elif len(matches) >= 4:\n",
    "            print(f\"Warning: Not enough matches for label {label}. Using all {len(matches)} matches.\")\n",
    "            pts_0 = np.float32([keypoints_0[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            pts_16 = np.float32([keypoints_16[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "            M, mask = cv2.estimateAffinePartial2D(pts_0, pts_16)\n",
    "        else:\n",
    "            print(f\"Not engough matches for label {label}. Using identical transform.\")\n",
    "            M = np.eye(2,3)\n",
    "\n",
    "        return M\n",
    "    \n",
    "    # Compute transforms for each label\n",
    "    transforms = {label: compute_transform(label, seg_map, gray_source, gray_target) for label in unique_labels}\n",
    "\n",
    "    return transforms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### global_motion_estimation: \n",
    "### 拿前後彩圖去做語意分割，再拿前後跟目標灰階圖去找特徵點並計算轉換矩陣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_motion_estimation(bf_path, af_path, bf_gray_path, af_gray_path, target_gray_path, model):\n",
    "    '''\n",
    "    bf_path, af_path: RGB image for segmentation, a string\n",
    "    bf_gray_path, af_gray_path, target_gray_path: gray image for feature extraction, a string\n",
    "    model: DeepLabModel object\n",
    "    '''\n",
    "\n",
    "    # read teh gray images\n",
    "    bf_gray = cv.imread(bf_gray_path)\n",
    "    af_gray = cv.imread(af_gray_path)\n",
    "    target_gray = cv.imread(target_gray_path)\n",
    "\n",
    "    bf_gray = cv.cvtColor(bf_gray, cv.COLOR_BGR2GRAY)\n",
    "    af_gray = cv.cvtColor(af_gray, cv.COLOR_BGR2GRAY)\n",
    "    target_gray = cv.cvtColor(target_gray, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "    # read the RGB images\n",
    "    bf_image = Image.open(bf_path)\n",
    "    af_image = Image.open(af_path)\n",
    "\n",
    "    # segment the images\n",
    "    bf_seg_map = model.run(bf_image)\n",
    "    af_seg_map = model.run(af_image)\n",
    "\n",
    "    # compute each M matrix for bf and af images' labels\n",
    "    sift = cv2.SIFT_create()\n",
    "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "\n",
    "    bf_M_dic = compute_M(bf_seg_map, bf_gray_path, target_gray_path, sift, bf)\n",
    "    af_M_dic = compute_M(af_seg_map, af_gray_path, target_gray_path, sift, bf)\n",
    "\n",
    "    # tansform\n",
    "    bf_predict = np.zeros_like(target_gray)\n",
    "    af_predict = np.zeros_like(target_gray)\n",
    "\n",
    "    for label, M in bf_M_dic.items():\n",
    "        mask = bf_seg_map == label\n",
    "        transformed_frame = cv2.warpAffine(bf_gray, M, (bf_gray.shape[1], bf_gray.shape[0]))\n",
    "        bf_predict[mask] = transformed_frame[mask]\n",
    "    \n",
    "    for label, M in af_M_dic.items():\n",
    "        mask = bf_seg_map == label\n",
    "        transformed_frame = cv2.warpAffine(af_gray, M, (af_gray.shape[1], af_gray.shape[0]))\n",
    "        af_predict[mask] = transformed_frame[mask]\n",
    "\n",
    "    # weighted sum\n",
    "    alpha = 0.5\n",
    "    target_predict = cv2.addWeighted(bf_predict, alpha, af_predict, 1 - alpha, 0)\n",
    "\n",
    "    return target_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 實際執行的程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf_path = './rgb_images/000.png'\n",
    "af_path = './rgb_images/032.png'\n",
    "\n",
    "bf_gray_path = './png/000.png'\n",
    "af_gray_path = './png/032.png'\n",
    "target_gray_path = './png/016.png'\n",
    "\n",
    "predict_frame16 = global_motion_estimation(bf_path, af_path, bf_gray_path, af_gray_path, target_gray_path, MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 L2 distance 計算成效如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_16 = cv2.imread(target_gray_path)\n",
    "frame_16 = cv.cvtColor(frame_16, cv.COLOR_BGR2GRAY)\n",
    "l2_distance_segments = np.linalg.norm(frame_16.astype(np.float32) - predict_frame16.astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
